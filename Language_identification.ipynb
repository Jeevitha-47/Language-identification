{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtjuXb-Xdrtg",
        "outputId": "79c30cb5-1584-42ea-d6bd-b0a30c974fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"Hello, how are you?\",\n",
        "        \"Bonjour, comment ça va?\",\n",
        "        \"Hola, ¿cómo estás?\",\n",
        "        \"Hallo, wie geht's dir?\",\n",
        "        \"Ciao, come stai?\",\n",
        "        \"Olá, como você está?\",\n",
        "        \"Привет, как дела?\",\n",
        "        \"こんにちは、お元気ですか？\",\n",
        "        \"你好，你怎么样？\",\n",
        "        \"안녕하세요, 어떻게 지내세요?\"\n",
        "    ],\n",
        "    \"language\": [\n",
        "        \"English\", \"French\", \"Spanish\", \"German\", \"Italian\",\n",
        "        \"Portuguese\", \"Russian\", \"Japanese\", \"Chinese\", \"Korean\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['language'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a pipeline with character n-gram vectorizer and Naive Bayes classifier\n",
        "model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char', ngram_range=(1, 3))),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Test on new input\n",
        "def identify_language(text):\n",
        "    return model.predict([text])[0]\n",
        "\n",
        "# Example\n",
        "print(identify_language(\"안녕하세요, 어떻게 지내세요?\"))  # Output should be \"  Korean\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oOcYLZRd-Us",
        "outputId": "4a30fcd5-71fa-49c2-f9d5-ee81ba01b339"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "Korean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langdetect\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQWvChjcecdW",
        "outputId": "9f6325d6-ff91-4bef-8115-1d0126a09628"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=fd4d8fd00f753ff904015bd3dc7633cb2df2ae216c8feb59d6fece4e6215b170\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect, DetectorFactory\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "# Ensure consistent results\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def identify_language(text):\n",
        "    try:\n",
        "        language = detect(text)\n",
        "        return language\n",
        "    except LangDetectException:\n",
        "        return \"Could not detect language\"\n",
        "\n",
        "# Example usage\n",
        "texts = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"Bonjour, comment allez-vous?\",\n",
        "    \"¿Dónde está la biblioteca?\",\n",
        "    \"Это моя книга.\",\n",
        "    \"これはペンです。\",\n",
        "    \"안녕하세요, 반갑습니다.\",\n",
        "    \"Buongiorno, come va?\",\n",
        "    \"مرحبا كيف حالك؟\"\n",
        "]\n",
        "\n",
        "for t in texts:\n",
        "    print(f\"Text: {t}\\nDetected Language: {identify_language(t)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8J2yxZyef8w",
        "outputId": "b80a876c-2852-42a7-a236-765f1da83f81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Hello, how are you?\n",
            "Detected Language: en\n",
            "\n",
            "Text: Bonjour, comment allez-vous?\n",
            "Detected Language: fr\n",
            "\n",
            "Text: ¿Dónde está la biblioteca?\n",
            "Detected Language: es\n",
            "\n",
            "Text: Это моя книга.\n",
            "Detected Language: ru\n",
            "\n",
            "Text: これはペンです。\n",
            "Detected Language: ja\n",
            "\n",
            "Text: 안녕하세요, 반갑습니다.\n",
            "Detected Language: ko\n",
            "\n",
            "Text: Buongiorno, come va?\n",
            "Detected Language: it\n",
            "\n",
            "Text: مرحبا كيف حالك؟\n",
            "Detected Language: ar\n",
            "\n"
          ]
        }
      ]
    }
  ]
}